---
published: true
layout: post
title: English version
---
### Introduction

When developing a text editor, it is necessary to determine certain technical parameters affecting the final product before we start. These things can include:

• The operating system, usage on mobile devices, usage by people with disabilities.

• Is integration with other software products on the market necessary?

• What is of higher priority - the speed of application loading, the speed of work in the running state, the size of the application, the size of files created with a text editor?

Consider one of the elements that need to be define before starting the process of developing an application - the internal encoding of the text files.

### History of creation

ASCII (American standard code for information interchange) is one of the first encodings created for the internal presentation of text files. It defines 256 characters (8 bits), 128 of them are standard, and the others began using for national encodings. Such a quantity of additional characters did not allow encoding of all the necessary characters, sometimes they required significantly more. There was also a problem with converting text files from one encoding to another. Unicode standard appeared in 1991 to solve this problem.

Unicode is a universal standard that allows encoding almost any character used in the modern world. Now it defines 137 994 characters, the last character added in May 2019.

### Coding system

The Unicode standard uses from 1 to 4 bytes to describe characters.

The simplest is the UTF-32 (Unicode Transformation Format) coding system, in which each character is set of 4 bytes. However, in fact, only 21 bits using to store characters (000000–10FFFF). The size of text files with using this encoding is large enough, that is why, usually two other encodings using: UTF-8 and UTF-16. The number in the title indicates the minimum quantum used to encode the character. For the UTF-8 version, this is 1 byte or 8 bits, so the character encoding with 1, 2, 3 or 4 bytes. For UTF-16, the character encoding with 2 or 4 bytes.

The coding system in UTF-8 based on the following principle:

Determine the minimum required number of bytes to encode this character (from 1 to 4).

| Range                | Quantity of bytes |
| :------------------- | ----------------: |
| 00000000–0000007F    | 1 byte            |
| 00000080–000007FF    | 2 bytes           |
| 00000800–0000FFFF    | 3 bytes           |
| 00010000–0010FFFF    | 4 bytes           |

The bit representation of the character inserting into the mask.

| Bytes             .| Number of Significant Bits   .| Pattern |
| :--------------    | :-----------    | :------------------------------------ |
| 1                  | 7               | 0xxxxxxx                              |
| 2                  | 11              | 110xxxxx 10xxxxxx                     |
| 3                  | 16              | 1110xxxx 10xxxxxx 10xxxxxx            |
| 4                  | 21              | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx   |

This encoding allows you to minimize the size of the text file, since ASCII characters, including Latin letters, numbers and basic punctuation marks, encoding with only 1 byte. If the next character starts with 0, then it is an ASCII character, and if it is 1, then the first byte can be easily distinguished, since its most significant bits are not equal to 10.

The coding for UTF-16 organized other way.

Most characters are 2 bytes in the ranges (0000 – D7FF) and (E000 – FFFF). The range (D800 – DFFF) using to encode characters from the range (10000–10FFFF). The coding mechanism is as follows:

1. 10000 subtracted from the symbol code. As a result, a value from 0 to FFFFF is obtained, which is placed in the 20-bit bit grid.

2. The upper 10 bits (a number in the range of 0000–03FF) adding with D800, and the result goes to the first word that is included in the D800 – DBFF range.

3. The low 10 bits (also a number in the range of 0000–03FF) adding with DC00, and the result goes to the second word, which is included in the range of DC00 – DFFF.

### Byte order

For UTF-16 encoding, a different sequence of high and low bytes is possible. Historically, this determined by the processor architecture, and now both options still used. A system compatible with x86 processors called little endian (LE), and with m68k and SPARC processors - big endian (BE). BOM (Byte order mark) uses to determine byte order in file. Label usage allows identify the encoding of the file.

| Coding        .| Representation (Hex)              |
| :------------: | :-------------------------------: |
| UTF-8          | EF BB BF                          |
| UTF-16 (BE)    | FE FF                             |
| UTF-16 (LE)    | FF FE                             |

### Comparing UTF-8 and UTF-16 encodings

Choosing the internal format for storing text files, some parameters are more important than others are. It is total file size, the speed of access to random location in the file, and speed and convenience of indexing internal data for search system.

Latin letters, numbers and basic punctuation marks encoding for UTF-8 by one byte and for UTF-16 by two bytes. In other way, if the range of characters (0800 – D7FF) and (E000 – FFFF) in which the characters of Chinese, Japanese and other non-European languages ​​are located is used, then these characters will occupy three bytes in UTF-8 encoding, and only two bytes in UTF-16.

The access speed to random location in file is approximately the same for both encodings, since both encodings have a variable character length.

For reading and indexing files, the algorithm for finding the character for UTF-8 is more complex and requires more system resources than UTF-16.

When using a text editor in a Windows environment, keep in mind that the UTF-16LE encoding is the internal Windows format described in the Win32 API.

For further information visit [http://unicode.org/](http://unicode.org/)
