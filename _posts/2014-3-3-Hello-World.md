---
layout: post
title: Тестовое задание для documentat.io
published: true
---

### Введение

При разработке текстового редактора необходимо на начальном этапе определиться с некоторыми техническими параметрами, влияющими на конечный продукт. К таким вещам можно отнести:

- Операционная система, использование на мобильных устройствах, будет ли этот продукт использоваться людьми с ограниченными возможностями.
- Необходима ли интеграция с другими программными продуктами, существующими на рынке.
- Что является более приоритетным – скорость загрузки приложения, скорость его работы в запущенном состоянии, размер приложения и тех файлов, которые создаются посредством текстового редактора.

Рассмотрим один из элементов, с которым важно определиться до начала процесса разработки приложения – внутренняя кодировка создаваемых текстовых файлов.

### История создания

Одной из первых кодировок, созданных для внутреннего представления текстовых файлов, является ASCII (Americanstandardcodeforinformationinterchange). В ней определяются 256 символов (8 бит), из которых 128 были стандартными, а остальные стали использовать для национальных кодировок. Такое количество дополнительных символов не всегда позволяли закодировать все нужные символы, иногда их требовалось существенно больше. Также возникала проблема с конвертацией текстовых файлов из одной кодировки в другую. Для решения этой проблемы в 1991 году было предложено использование стандарта Unicode.

Unicode является универсальным стандартом, позволяющим закодировать практически любые символы, используемые в современном мире. На данный момент закодировано 137 994 символов, последний символ был добавлен в мае 2019 года.

### Система кодирования

В общем варианте стандарт Unicode использует от 1 до 4 байт для описания символов.

Наиболее простой можно считать систему кодирования UTF-32 (Unicode Transformation Format), в которой каждый символ записывается набором из 4 байт. Однако, фактически, для хранения символов используется только 21 бит (000000–10FFFF).

Размер текстовых файлов при использовании такой кодировки достаточно велик, поэтому, в основном, используются две других кодировки: UTF-8 и UTF-16. Цифра в названии обозначает минимальный квант, используемый для кодирования знака. Для версии UTF-8 это 1 байт или 8 бит, таким образом символ может быть закодирован 1, 2, 3 или 4 байтами. Для UTF-16 символ может кодироваться 2 или 4 байтами.

Система кодирования в UTF-8 основана на следующем принципе:

Определяется минимально необходимое количество байт для кодирования данного символа (от 1 до 4).

| Диапазон | Количество байт |
| --- | --- |
| 00000000–0000007F | 1 байт |
| 00000080–000007FF | 2 байта |
| 00000800–0000FFFF | 3 байта |
| 00010000–0010FFFF | 4 байта |

Битовое представление символа заносится в маску.

| Количество байт | Значащих бит | Шаблон |
| --- | --- | --- |
| 1 | 7 | 0xxxxxxx |
| 2 | 11 | 110xxxxx 10xxxxxx |
| 3 | 16 | 1110xxxx 10xxxxxx 10xxxxxx |
| 4 | 21 | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx |

Такое кодирование позволяет минимизировать размер текстового файла, так как символы ASCII, в том числе латинские буквы, цифры и основные знаки препинания, кодируются лишь 1 байтом. Если очередной символ начинается с 0, то это символ ASCII, а если с 1, то можно легко отличить первый байт, так как его старшие биты не равны 10.

Кодирование для UTF-16 организовано по другому принципу.

Большинство символов задаются 2 байтами в диапазонах (0000–D7FF) и (E000–FFFF). Диапазон (D800–DFFF) используется для кодирования символов из диапазона (10000–10FFFF). Механизм кодирования следующий:

1. Из кода символа вычитается 10000. В результате получится значение от 0 до FFFFF, которое помещается в разрядную сетку 20 бит.
2. Старшие 10 бит (число в диапазоне 0000–03FF) суммируются с D800, и результат идёт в первое слово, которое входит в диапазон D800–DBFF.
3. Младшие 10 бит (тоже число в диапазоне 0000–03FF) суммируются с DC00, и результат идёт во второе слово, которое входит в диапазон DC00–DFFF.

### Порядок байт

Для кодировки UTF-16 возможна различная последовательность старшего и младшего байтов. Исторически это определялось архитектурой процессора, на данный момент продолжают использоваться оба варианта. Систему, совместимую с процессорами x86, называют little endian(LE), а с процессорами m68k и SPARC — big endian(BE). Для определения того, как расположены байты в файле, используется метка порядка байтов BOM (Byte order mark). Использование такой метки позволяет достаточно уверенно идентифицировать кодировку файла.

| Кодирование | Представление (Шестнадцатеричное) |
| --- | --- |
| UTF-8 | EF BB BF |
| UTF-16 (BE) | FE FF |
| UTF-16 (LE) | FF FE |

### Сравнение кодировок UTF-8 и UTF-16

При определении внутреннего формата для хранения текстовых файлов важными являются такие параметры как общий объем файла, скорость доступа к произвольному месту в файле и скорость и удобство индексации внутренних данных для системы поиска.

Латинские буквы, цифры и основные знаки препинания кодируется для UTF-8 одним байтом, а для UTF-16 – двумя. С другой стороны, если используется диапазон символов (0800–D7FF) и (E000–FFFF), в котором расположены символы китайского, японского и других неевропейских языков, то эти символы будут занимать в кодировке UTF-8 три байта, а в UTF-16 только два байта.

Скорость доступа к произвольному месту в файле примерно одинакова для обеих кодировок, так как обе кодировки имеют переменную длину символа.

При чтении и дальнейшей индексации файлов алгоритм определения символа для отображения в варианте UTF-8 более сложен и требует больше системных ресурсов, чем UTF-16.

При использовании текстового редактора в среде Windows следует учитывать, что кодировка UTF-16LE является внутренним форматом Windows, описанным в API Win32.

Для получения дополнительной информации посетите [http://unicode.org/](http://unicode.org/)
